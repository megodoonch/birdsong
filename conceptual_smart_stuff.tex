\documentclass[12pt]{article}
\usepackage{mathptmx}% http://ctan.org/pkg/mathptmx
\usepackage{times}
\usepackage{amsmath}
\usepackage{amsthm}
 \usepackage{amsfonts}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\title{People who want to parse bigrams and finite state machines\\good}
\author{Meaghan ``geitje'' Fowlie and Floris ``konijntje'' van Vugt}


\begin{document}

\maketitle

\section{Definitions}

%\newcommand\STATES{\mathcal{S}}
\newcommand\STATES{\mathbb{S}}
%\newcommand\OPS{\mathcal{O}}
\newcommand\OPS{\mathbb{O}}
%\newcommand\BIGR{\mathcal{B}}
\newcommand\BIGR{\mathbb{B}}
\newcommand\FSA{\textsc{FSA}}
%\newcommand\PARSES{\mathcal{P}}
\newcommand\PARSES{\mathbb{P}}
\newcommand\SC{\textsc{SC}}
\newcommand\TC{\textsc{TC}}
\newcommand\N{\mathbb{N}}

The set of all possible states is $\STATES$ and the set of all operations is $\OPS$ and the set of all bigrams is $\BIGR$



\begin{definition}[finite state machines]
  A finite state automaton (Meaghan TODO) $\FSA$
  Set of tuples $(q_i,e,q_j)$ where $q_i,q_j\in\STATES$ and $e\in\OPS$ meaning that you can transition from state $q_i$ to $q_j$ by emitting $e$.
\end{definition}

\begin{definition}[transition probabilities]
  A probability assignment $\phi$ is a function from elements of the finite state automaton (i.e. transitions) to probabilities, such that the sum of all probabilities with the same left hand side is 1 (TODO: make that precise).
\end{definition}


\begin{definition}[route]
  A \emph{route} is a route through the $\FSA$ of say $n$ steps, defined as a tuple $(Q,E)$ where $Q$ is the sequence of states visited, i.e.
  $Q=(q_i\in\STATES|i<n)$, and $E$ is the sequence of emissions, i.e. $E=(e_i\in\OPS|i<n-1)$.
\end{definition}


\begin{definition}[parse]
  A \emph{parse} is what Meaghan's fanatic function currently outputs, more or less.
  Given a sentence $s$ we define a set $\PARSES(s) = \{ (b,r) \}$ where each $b$ is a sequence of alphabet elements (bigrams), and $r$ is a route through the operations $\FSA$.
\end{definition}


\begin{definition}[state counts]
  Given a route $(Q,E)$, we define $\SC_Q : \STATES \rightarrow\N$ as follows: $\SC_Q(q) = \#\{ q'\in \STATES | q'=q \}$
\end{definition}


\begin{definition}[transition counts]
  Given a route $(Q,E)$, we define $\TC_{Q,E} : \FSA \rightarrow\N$ as follows: $\TC_{Q,E}(q,e,q') = \#\{ i<n | Q(i)=q,E(i)=e,Q({i+1})=q'\}$
\end{definition}


\section{Likelihoods}

\begin{definition}[rule probability assignment]
A rule probability assignment is an assignment of a probability to each transition, i.e. $p(q,e,q')$, such that $\sum_{q'}p(q,e,q')=1$
\end{definition}

\begin{definition}[likelihoods]
A rule probability assignment can then be extended in a straightforward way to yield, given a sentence $s$ the probability of the sentence $p(s)$ and by extension given a corpus $C$ the probability of the corpus $p(C)=\prod_{s\in C}p(s)$.
\end{definition}

So our task is given a corpus $C$ to find a probability assignment that maximises $p(C)$. 

Since the search space is large, it may only be possible to find a local maximum and not a global maximum.

We're somewhat unsure how the following achieves this, but it seems to work.


\section{Iterative rule updates}


We want to estimate the likelihood of each rule in a given corpus. For this, we iteratively re-estimate the rule probabilities: given a rule probability assignment, we can re-estimate the probabilities of every parse of a sentence, and then this changes the probabilities that each rule was used.

Roughly speaking, a transition from state $q$ to $q'$ can be estimated to happen with the following likelihood:
$$\phi(q,q') = \frac{\textrm{expected number of times we went }q\rightarrow q'}{\textrm{expected number of times we were in }q}$$

This leads to the following definition:

\begin{definition}[sentence-level update rule]
  Given a sentence $s$ and given a probability assignment $\phi$ we can define an updated probability assignment $\phi'$ as follows:

  $$\phi_s'(q,e,q') = \frac{\sum_{(b,r)\in\PARSES(s)}p_\phi(b,r)~\TC_r(q,e,q')}{\sum_{(b,r)\in\PARSES(s)}p_\phi(b,r)~\SC_r(q)}$$
\end{definition}

%\begin{definition}[sentence-level update rule]
%  Given a sentence $s$ and given a probability assignment $\phi$ we can define an updated probability assignment $\phi'$ as follows:%
%
%  $$\phi_s'(q,e,q') = \sum_{(b,r)\in\PARSES(s)}\frac{p_\phi(b,r)}{\sum_{(b',r')\in\PARSES(s)}p_\phi(b',r')}\frac{\TC_r(q,e,q')}{\SC_r(q)}$$
%\end{definition}

%We can write more simply $p_\phi(s)=\sum_{(b',r')\in\PARSES(s)}p_\phi(b',r')$ so that

%$$\phi_s'(q,e,q') = \frac{1}{p(s)}\sum_{(b,r)\in\PARSES(s)}p(b,r)\frac{\TC_r(q,e,q')}{\SC_r(q)}$$


Similarly, given a corpus, we compute the updates based on all parses in parallel:
\begin{definition}[corpus update rule]
  Given a corpus $C$ and given a probability assignment $\phi$ we can define an updated probability assignment $\phi'$ as follows:

 $$\phi_C'(q,e,q') = \frac{\sum_{s\in C}\sum_{(b,r)\in\PARSES(s)} p_\phi(b,r)~ \TC_r(q,e,q')}{\sum_{s\in C}\sum_{(b,r)\in\PARSES(s)} p_\phi(b,r)~ \SC_r(q)}$$ 
\end{definition}

%Floris' version:
%$$\phi_C'(q,e,q') = \frac{\sum_{s\in C}p(s)\phi_s'(q,e,q')}{\sum_{s\in C}p(s)}$$ 


The sums can't be combined into one sum because sometimes there are parses that visit state $q$ but don't follow transition $(q,e,q')$. However, we still want that visit in the state counts for that transition.

We can't use the probability of the whole corpus because not all sentences have parses that visit all states.





TODO:
\begin{itemize}
\item Prove correctness (rule probabilities with the same left hand side sum to one)
\item Prove that if there is no ambiguity then the updated probability rules do not depend on the initial rule assignments (i.e. you converge in one step).
\item Prove that each rule update increases the likelihood of the corpus.
\item Prove convergence.
\end{itemize}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
